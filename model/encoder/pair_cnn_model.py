import torch
import torch.nn as nn
from typing import Optional

# CNNEncoder í´ë˜ìŠ¤ë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
from models.st_mem_1dcnn import CNNEncoder 


class PairDiffCNN(nn.Module):
    """
    1D CNN ì¸ì½”ë”ë¥¼ ê³µìœ í•˜ë©°, ê° ì‹œì ì˜ íŠ¹ì§•(256D)ì„ ìŠ¤ì¹¼ë¼ ì ìˆ˜ë¡œ ì••ì¶•í•œ í›„,
    ê·¸ ìŠ¤ì¹¼ë¼ ì ìˆ˜ì˜ ì°¨ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.
    """

    def __init__(self, num_classes=2, embed_dim=256, **kwargs):
        super().__init__()

        D_out = embed_dim  # 256

        # 1. Base ì¸ì½”ë” ë¡œë“œ (CNNEncoderë¥¼ ë¡œë“œ)
        # Note: CNNEncoderëŠ” headë¥¼ í¬í•¨í•˜ë©°, forward_featuresë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
        self.encoder_body = CNNEncoder(num_classes=num_classes, embed_dim=embed_dim, **kwargs)

        # ğŸ“Œ ìˆ˜ì •: Score Head -> self.diff_head
        self.diff_head = nn.Sequential(
            nn.Linear(D_out, D_out // 2),  # 256 -> 128
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(D_out // 2, 1)  # 128 -> 1 (ë‹¨ì¼ ìŠ¤ì¹¼ë¼ ì ìˆ˜)
        )

        # ğŸ“Œ ìˆ˜ì •: Final Head -> self.diff_head2
        self.diff_head2 = nn.Sequential(
            nn.Linear(256, 512),  # 1 -> 64
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, num_classes)  # 64 -> 2 (í˜¸ì „/ê·¸ ì™¸ ë¡œì§“)
        )
        self.attn = nn.MultiheadAttention(embed_dim=256, num_heads=4, batch_first=True)
        
    def forward(self, x_pre: torch.Tensor, x_post: torch.Tensor) -> torch.Tensor:
        # CNNEncoderì˜ forward_featuresë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. (ì´ ë¡œì§ì€ ë³„ë„ íŒŒì¼ì— ì¶”ê°€ë˜ì–´ì•¼ í•¨)
        # 1. ì¸ì½”ë” ê³µìœ  ë° íŠ¹ì§• ë²¡í„° ì¶”ì¶œ (256D)
        z_pre = self.encoder_body.forward_features(x_pre)  # íŠ¹ì§• ë²¡í„° [B, 256]
        z_post = self.encoder_body.forward_features(x_post)  # íŠ¹ì§• ë²¡í„° [B, 256]
        
        query = z_post.unsqueeze(1) # [B, 1, 256]
        key = z_pre.unsqueeze(1)    # [B, 1, 256]
        value = z_pre.unsqueeze(1)  # [B, 1, 256]
        # 2. ì–´í…ì…˜ ì—°ì‚°
        attn_output, _ = self.attn(query, key, value)

        # 3. ì°¨ì› ë³µêµ¬ ë° ì”ì°¨ ì—°ê²° (ì„ íƒì‚¬í•­ì´ì§€ë§Œ ë³´í†µ ì›ë³¸ì„ ë”í•´ì¤Œ)
        D = attn_output.squeeze(1) + z_post
        # 4. ìµœì¢… MLP ë¶„ë¥˜
        
        logits = self.diff_head2(D) # self.diff_head2 ì‚¬ìš©

        return logits
'''import torch
import torch.nn as nn
# ìœ„ì—ì„œ ì •ì˜í•˜ì‹  TCNEncoderë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
from models.st_mem_tcn import TCNEncoder 

class PairDiffTCN(nn.Module):
    """
    Pretrained TCN Encoderë¥¼ ì‚¬ìš©í•˜ì—¬ ë‘ ì‹œì ì˜ ì°¨ì´ë¥¼ ë¶„ì„í•˜ëŠ” ëª¨ë¸
    """
    def __init__(self, num_classes=2, embed_dim=256, **kwargs):
        super().__init__()
        
        # 1. TCN Encoder ë¡œë“œ
        self.encoder_body = TCNEncoder(embed_dim=embed_dim, **kwargs)
        
        # ğŸ“Œ ì¶”ê°€ëœ ë¶€ë¶„: Global Average Pooling
        # TCNì€ ì‹œê°„ ì¶•(L)ì´ ì‚´ì•„ìˆëŠ” [B, C, L]ì„ ì¶œë ¥í•˜ë¯€ë¡œ, 
        # ì´ë¥¼ í•˜ë‚˜ì˜ ë²¡í„° [B, C]ë¡œ ì••ì¶•í•´ì•¼ í•©ë‹ˆë‹¤.
        self.gap = nn.AdaptiveAvgPool1d(1)

        D_out = embed_dim  # 256

        # 2. Score Head (íŠ¹ì§• ë²¡í„° -> ìŠ¤ì¹¼ë¼ ì ìˆ˜)
        self.diff_head = nn.Sequential(
            nn.Linear(D_out, D_out // 2),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(D_out // 2, 1)  # ìŠ¤ì¹¼ë¼ ì ìˆ˜ ì¶œë ¥
        )

        # 3. Final Head (ì ìˆ˜ ì°¨ì´ -> í˜¸ì „ ì—¬ë¶€ ë¡œì§“)
        self.diff_head2 = nn.Sequential(
            nn.Linear(1, 64),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(64, num_classes)
        )

    def forward(self, x_pre, x_post):
        # 1. TCN Feature Extraction: [B, 256, L]
        f_pre = self.encoder_body.forward_features(x_pre)
        f_post = self.encoder_body.forward_features(x_post)

        # 2. Pooling: [B, 256, L] -> [B, 256, 1] -> [B, 256]
        z_pre = self.gap(f_pre).squeeze(-1)
        z_post = self.gap(f_post).squeeze(-1)

        # 3. Score Calculation
        score_pre = self.diff_head(z_pre)
        score_post = self.diff_head(z_post)

        # 4. Difference & Classification
        D_scalar = score_post - score_pre
        logits = self.diff_head2(D_scalar)

        return logits'''
import torch
import torch.nn as nn
from models.st_mem_tcn import TCNEncoder # ê¸°ì¡´ TCN ì¸ì½”ë” ì‚¬ìš©

class PairDiffTCN_Feature(nn.Module):
    """
    TCN Encoderë¥¼ ì‚¬ìš©í•˜ì—¬ Feature Vectorë¥¼ ì¶”ì¶œí•˜ê³ ,
    (Post Feature - Pre Feature)ì˜ ì°¨ì´ ë²¡í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸
    """
    def __init__(self, num_classes=2, embed_dim=256, **kwargs):
        super().__init__()
        
        # 1. TCN Encoder ë¡œë“œ (Weights ê³µìœ )
        # kwargsë¡œ kernel_size, dropout ë“±ì„ ì „ë‹¬ë°›ìŠµë‹ˆë‹¤.
        self.encoder_body = TCNEncoder(embed_dim=embed_dim, **kwargs)
        
        # 2. Global Average Pooling
        # [B, 256, L] -> [B, 256, 1]
        self.gap = nn.AdaptiveAvgPool1d(1)

        # 3. Classifier Head (Feature Difference -> Class Logits)
        # ì…ë ¥ ì°¨ì›ì´ '1'ì´ ì•„ë‹ˆë¼ 'embed_dim(256)'ì´ ë©ë‹ˆë‹¤.
        self.diff_head = nn.Sequential(
            nn.Linear(embed_dim, embed_dim // 2), # 256 -> 128
            nn.BatchNorm1d(embed_dim // 2),       # ì•ˆì •ì ì¸ í•™ìŠµì„ ìœ„í•œ BN ì¶”ê°€
            nn.ReLU(),
            nn.Dropout(0.5),
            
            nn.Linear(embed_dim // 2, 64),        # 128 -> 64
            nn.ReLU(),
            nn.Dropout(0.5),
            
            nn.Linear(64, num_classes)            # 64 -> 2 (0 or 1)
        )

        # ğŸ“Œ [í•µì‹¬] ì´ˆê¸°í™” ê¿€íŒ ì ìš© (Bias 0)
        # ì´ˆê¸° ì¶œë ¥ì„ 0 ê·¼ì²˜ë¡œ ë§Œë“¤ì–´ 50:50 í™•ë¥ ì—ì„œ ì‹œì‘í•˜ê²Œ í•¨
        nn.init.constant_(self.diff_head[-1].bias, 0.0)
        nn.init.normal_(self.diff_head[-1].weight, std=0.01)

    def forward(self, x_pre, x_post):
        # 1. TCN Feature Extraction: [B, 256, L]
        # weights sharing (ìƒ´ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°)
        f_pre = self.encoder_body.forward_features(x_pre)
        f_post = self.encoder_body.forward_features(x_post)

        # 2. Pooling: [B, 256, L] -> [B, 256]
        z_pre = self.gap(f_pre).squeeze(-1)
        z_post = self.gap(f_post).squeeze(-1)

        # 3. Feature Difference (ë²¡í„° ë¹¼ê¸°)
        # ìŠ¤ì¹¼ë¼ê°€ ì•„ë‹ˆë¼ 256ì°¨ì› ë²¡í„°ë¼ë¦¬ì˜ ì°¨ì´ì…ë‹ˆë‹¤.
        diff_vector = z_post - z_pre  # [B, 256]

        # 4. Classification
        logits = self.diff_head(diff_vector)

        return logits